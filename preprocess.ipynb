{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\abdeslem\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\abdeslem\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\abdeslem\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\abdeslem\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\abdeslem\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\abdeslem\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdeslem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import nltk\n",
    "import keras as k\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Dense, merge, Dropout, RepeatVector, Activation, merge, Lambda, Flatten, Reshape\n",
    "from keras.layers import LSTM, Bidirectional, TimeDistributed, GRU, concatenate\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import string\n",
    "import argparse\n",
    "from collections import Counter\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "# from utils copy preprocess.py and attention.py to colab local location\n",
    "from utils.preprocess import splitDatasets, splitValDatasets,vectorizeValData, vectorizeData\n",
    "from utils.attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove/glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train-v2.0.json') as json_data:\n",
    "    d = json.load(json_data)\n",
    "trainData = d['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tContext, tQuestion, tQuestion_id, tAnswerBegin, tAnswerEnd, tAnswerText, maxLenTContext, maxLenTQuestion = splitDatasets(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('data/dev-v2.0.json') as json_data:\n",
    "    d = json.load(json_data)\n",
    "valData = d['data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vContext, vQuestion, vQuestion_id, vAnswerBegin, vAnswerEnd, vAnswerText, vmaxLenTContext, vmaxLenTQuestion = splitDatasets(valData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for words in tContext + tQuestion + vContext + vQuestion:\n",
    "    for word in words:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "vocab = sorted(vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106995\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab) + 1\n",
    "print(vocab_size)\n",
    "word_index = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "context_maxlen = max(maxLenTContext, vmaxLenTContext)\n",
    "question_maxlen = max(maxLenTQuestion, vmaxLenTQuestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX, tXq, tYBegin, tYEnd = vectorizeData(tContext, tQuestion, tAnswerBegin, tAnswerEnd, word_index, context_maxlen, question_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vX, vXq, vYBegin, vYEnd = vectorizeData(vContext, vQuestion, vAnswerBegin, vAnswerEnd, word_index, context_maxlen, question_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = len(word_index)\n",
    "EMBEDDING_DIM = 100\n",
    "MAX_SEQUENCE_LENGTH = context_maxlen\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words+1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\abdeslem\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "question_input = Input(shape=(question_maxlen,), dtype='int32', name='question_input')\n",
    "context_input = Input(shape=(context_maxlen,), dtype='int32', name='context_input')\n",
    "questionEmbd = Embedding(output_dim=EMBEDDING_DIM, input_dim=vocab_size,mask_zero=True, weights=[embedding_matrix], \n",
    "                         input_length=question_maxlen, trainable=False)(question_input)\n",
    "contextEmbd = Embedding(output_dim=EMBEDDING_DIM, input_dim=vocab_size,mask_zero=True, weights=[embedding_matrix], \n",
    "                input_length=context_maxlen, trainable=False)(context_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q = Bidirectional(LSTM(128, return_sequences=True))(questionEmbd)\n",
    "D = Bidirectional(LSTM(128, return_sequences=True))(contextEmbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2 = Attention(question_maxlen)(Q)\n",
    "D2 = Attention(context_maxlen)(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = concatenate([D2, Q2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerPtrBegin_output = Dense(context_maxlen, activation='softmax')(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lmerge = concatenate([L, answerPtrBegin_output],axis = 1)\n",
    "answerPtrEnd_output = Dense(context_maxlen, activation='softmax')(Lmerge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdeslem\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "context_input (InputLayer)      (None, 766)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "question_input (InputLayer)     (None, 60)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 766, 100)     10699500    context_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 60, 100)      10699500    question_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 766, 256)     234496      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 60, 256)      234496      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 256)          1022        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          316         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           attention_2[0][0]                \n",
      "                                                                 attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 766)          392958      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1278)         0           concatenate_1[0][0]              \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 766)          979714      concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 23,242,002\n",
      "Trainable params: 1,843,002\n",
      "Non-trainable params: 21,399,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(input=[context_input, question_input], output=[answerPtrBegin_output, answerPtrEnd_output])\n",
    "am = optimizers.Adam(lr=0.0005)\n",
    "model.compile(optimizer=am, loss='categorical_crossentropy',\n",
    "              loss_weights=[.04, 0.04], metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "49280/86821 [================>.............] - ETA: 2:08:46 - loss: 0.4112 - dense_1_loss: 5.1240 - dense_2_loss: 5.1569 - dense_1_accuracy: 0.0243 - dense_2_accuracy: 0.0137"
     ]
    }
   ],
   "source": [
    "train_slice = 10000\n",
    "filepath=\"modelweightsfinal.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "model_history = model.fit([tX, tXq], [tYBegin, tYEnd],batch_size= 128, verbose=1,\n",
    "                          callbacks = callbacks_list,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
